Interview qns-

Difference between row number, rank, denserank ?
Difference between Parquet and CSV file formats?
What's Mounting Storage location?
What's Spark Performance tuning and databricks performance tuning?
What's the reason to not use natural join in sql ?
What are scalar sub query, multiple row subquery and correlated subquery?
What's Broadcast join in spark?
Difference between repartition and coalesce ?
Explain the logical flow of execution of SQL query ?
Difference b/w drop, delete and truncate ?
What are wild cards in SQL ?
What are the different types of clusters we have in Azure Databricks?
What is the purpose of the COALESCE() function in sql?
How do you optimize SQL queries for better performance?
Difference between window functions and aggregate functions in sql ?


Q1. Basic questions on Spark and Scala. Scenario based questions on AWS Lambda and data pipeline


✅ Can you share your experience working with PySpark and processing large-scale data?
✅ How does PySpark fit within the Apache Spark ecosystem, and what are its benefits for distributed data processing?
✅ What are the main differences between DataFrames and RDDs in PySpark?
Could you explain the concepts of transformations and actions in PySpark DataFrames?
✅ What are some common operations you frequently perform on PySpark DataFrames?
✅ How do you go about optimizing the performance of PySpark jobs?
✅ What strategies do you use to handle skewed data in PySpark?
✅ How do you manage missing or null values in PySpark DataFrames?
✅ Are there any specific methods or approaches you prefer when handling missing data?
✅ How do you run SQL queries on PySpark DataFrames?
✅ Can you explain what broadcasting is and why it’s useful in PySpark?
✅ Could you provide an example where using broadcasting significantly boosts performance?
✅ How do you monitor and troubleshoot PySpark jobs effectively? What role does logging play in PySpark applications, and why is it important?







How do you invoke one notebook from another in Databricks?
What methods do you use to access a variable from one notebook in another?
How do you exit a notebook while returning output data in Databricks?
Can you explain the process of creating internal and external tables in Databricks?
What optimization techniques have you implemented in Spark?
How do you manage failure notifications in your workflows?
What is your approach to reprocessing data in case of a failure?
What are some of the lesser-known disadvantages of using Spark?
Can you explain the concept of the JVM and Python wrapper in Spark?
Why is it generally advised against using user-defined functions and data structures in Spark?
What are the drawbacks of using user-defined functions in Spark?
Could you explain the concept of Resilient Distributed Datasets (RDD) in PySpark?
What are actions and transformations in PySpark, and how do they differ?
How do you manage and handle null values in PySpark DataFrames?
What is a partition in PySpark, and how do you control partitioning for better performance?
Can you explain the difference between narrow and wide transformations in PySpark?
How does PySpark infer schemas, and what are the implications of this?
What role does SparkContext play in a PySpark application?
How do you perform aggregations in PySpark, and what are the key considerations?
What strategies do you use for caching data in PySpark to improve performance?



Can you explain the differences between a DataFrame and an RDD in PySpark?
What techniques would you use to optimize the performance of PySpark code?
How does the Catalyst Optimizer contribute to query execution in PySpark?
Which serialization formats are commonly used in PySpark, and why?
How do you address skewed data issues in PySpark?
Could you describe how memory management is handled in PySpark?
What are the different types of joins in PySpark, and how do you implement them?
What is the purpose of the broadcast() function in PySpark, and when should it be used?
How do you define and use User-Defined Functions (UDFs) in PySpark?
What is lazy evaluation in PySpark, and how does it affect job execution?
What are the steps to create a DataFrame in PySpark?
Could you explain the concept of Resilient Distributed Datasets (RDD) in PySpark?
What are actions and transformations in PySpark, and how do they differ?
How do you manage and handle null values in PySpark DataFrames?
What is a partition in PySpark, and how do you control partitioning for better performance?
Can you explain the difference between narrow and wide transformations in PySpark?
How does PySpark infer schemas, and what are the implications of this?
What role does SparkContext play in a PySpark application?
How do you perform aggregations in PySpark, and what are the key considerations?
What strategies do you use for caching data in PySpark to improve performance?




Describe the shuffle operation in Apache Spark and its impact on performance.
What are the different types of joins available in Apache Spark SQL?
Provide examples of when to use each. Discuss the optimizations performed by Apache Spark’s Catalyst optimizer.
Explain how broadcast variables work in Apache Spark and when they should be used.
How does Apache Spark handle memory management and garbage collection in its execution model?
Describe the architecture of Apache Spark and its components in a distributed environment. What are the different deployment modes available for running Apache Spark applications? When would you choose each mode?
Explain the role of the SparkContext in an Apache Spark application and how it differs from the SparkSession.
Discuss the performance tuning techniques you would employ to optimize Apache Spark jobs.
How does Apache Spark handle skewed data when performing aggregations or group-bys? Explain the concept of window functions in Apache Spark SQL and provide examples of their usage.
Discuss the role of lineage, checkpoints, and RDD persistence in ensuring fault tolerance in Apache Spark.